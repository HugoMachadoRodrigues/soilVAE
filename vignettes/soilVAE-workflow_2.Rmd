---
title: "soilVAE workflow (v2): PLS baseline vs. supervised VAE"
output:
  rmarkdown::html_vignette:
    toc: true
    toc_depth: 3
vignette: >
  %\VignetteIndexEntry{soilVAE workflow (v2): PLS baseline vs. supervised VAE}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width = 7,
  fig.height = 5
)
set.seed(19101991)
```

## Goal

This vignette reproduces a **classic soil spectroscopy workflow** (similar to the PLS example from the soil spectroscopy literature) and compares it to a **supervised VAE regression** implemented in **`soilVAE`**.

We keep the structure:

1)  Load spectra + soil property\
2)  Pre-process spectra (absorbance, resample, SNV, smoothing)\
3)  Split into calibration / validation\
4)  Fit **PLS** baseline\
5)  Fit **soilVAE** supervised VAE\
6)  Compare metrics (ME, MAE, RMSE, R², RPIQ, RPD)

### Improvements in this v2

We apply two changes (A and B), which typically make deep models behave much better in spectroscopy:

**A) Standardize** predictors (and optionally the target) based on the training set only (no leakage).\
**B) Make the VAE training more stable** via **(i)** a wider hyperparameter search and **(ii)** multiple random restarts per configuration (pick the best).

> Note: On CRAN/CI machines TensorFlow/Keras may be unavailable. The VAE section is guarded and will **skip gracefully** if TF/Keras are not installed.

------------------------------------------------------------------------

## Load data

This vignette expects a dataset named `datsoilspc` shipped with **this package** (file `data/datsoilspc.rda`).

```{r load-data}
data("datsoilspc", package = "soilVAE")
str(datsoilspc)
```

We assume:

-   `datsoilspc$spc` is a matrix-like object with reflectance values (rows = samples; cols = wavelengths)
-   `datsoilspc$TotalCarbon` is the target (numeric)

```{r check-fields}
stopifnot(!is.null(datsoilspc$spc))
stopifnot("TotalCarbon" %in% names(datsoilspc))

dim(datsoilspc$spc)
summary(datsoilspc$TotalCarbon)
```

------------------------------------------------------------------------

## Spectral pre-processing

We follow a common pipeline:

1.  Reflectance → **Absorbance**: `A = log(1/R)`
2.  **Resample** to 5 nm spacing
3.  **SNV** (baseline correction)
4.  **Moving average** smoothing

### Helpers

```{r helpers-prep}
snv <- function(X) {
  X <- as.matrix(X)
  mu <- rowMeans(X, na.rm = TRUE)
  sdv <- apply(X, 1, stats::sd, na.rm = TRUE)
  sdv[sdv == 0] <- 1
  (X - mu) / sdv
}

# Moving average along wavelengths (columns)
movavg_cols <- function(X, k = 11L) {
  X <- as.matrix(X)
  k <- as.integer(k)
  if (k < 1L) return(X)
  if (k %% 2L == 0L) stop("k must be odd (e.g., 11).")
  half <- (k - 1L) / 2L
  n <- ncol(X)
  out <- X
  for (j in seq_len(n)) {
    lo <- max(1L, j - half)
    hi <- min(n, j + half)
    out[, j] <- rowMeans(X[, lo:hi, drop = FALSE], na.rm = TRUE)
  }
  out
}

# Metrics used in soil spectroscopy papers
metrics <- function(y, yhat) {
  y <- as.numeric(y)
  yhat <- as.numeric(yhat)
  ok <- is.finite(y) & is.finite(yhat)
  y <- y[ok]; yhat <- yhat[ok]
  me <- mean(yhat - y)
  mae <- mean(abs(yhat - y))
  rmse <- sqrt(mean((yhat - y)^2))
  ss_res <- sum((y - yhat)^2)
  ss_tot <- sum((y - mean(y))^2)
  r2 <- if (ss_tot == 0) NA_real_ else 1 - ss_res / ss_tot
  rpiq <- stats::IQR(y, na.rm = TRUE) / rmse
  rpd  <- stats::sd(y, na.rm = TRUE) / rmse
  data.frame(ME = me, MAE = mae, RMSE = rmse, R2 = r2, RPIQ = rpiq, RPD = rpd)
}
```

### Pre-process spectra

For resampling we use `prospectr::resample()` if available. If not available, we keep original wavelengths.

```{r preprocess}
spcR <- as.matrix(datsoilspc$spc)
wavs <- suppressWarnings(as.numeric(colnames(spcR)))
if (any(!is.finite(wavs))) {
  stop("Column names of datsoilspc$spc must be numeric wavelengths.")
}

# 1) reflectance -> absorbance
spcA <- log(1 / pmax(spcR, 1e-8))

# 2) resample to 5 nm (if prospectr available)
if (requireNamespace("prospectr", quietly = TRUE)) {
  oldW <- wavs
  newW <- seq(min(oldW), max(oldW), by = 5)
  spcA_rs <- prospectr::resample(spcA, wav = oldW, new.wav = newW, interpol = "linear")
  colnames(spcA_rs) <- as.character(newW)
} else {
  spcA_rs <- spcA
}

# 3) SNV
spcA_snv <- snv(spcA_rs)

# 4) moving average (k=11)
spcA_mov <- movavg_cols(spcA_snv, k = 11)

dim(spcA_mov)
```

Optional: plot a few spectra (lightweight).

```{r plot-spectra, eval=TRUE}
w_plot <- as.numeric(colnames(spcA_mov))
matplot(
  x = w_plot,
  y = t(spcA_mov[seq_len(min(50, nrow(spcA_mov))), , drop = FALSE]),
  xlab = "Wavelength (nm)",
  ylab = "Absorbance (processed)",
  type = "l",
  lty = 1,
  col = grDevices::rgb(0.5, 0.5, 0.5, 0.25)
)
```

------------------------------------------------------------------------

## Split: calibration vs validation

```{r split}
set.seed(19101991)
n <- nrow(spcA_mov)
calId <- sample(seq_len(n), size = round(0.75 * n))
valId <- setdiff(seq_len(n), calId)

X_tr_raw <- spcA_mov[calId, , drop = FALSE]
X_va_raw <- spcA_mov[valId, , drop = FALSE]

y_tr_raw <- datsoilspc$TotalCarbon[calId]
y_va_raw <- datsoilspc$TotalCarbon[valId]

c(n_tr = length(y_tr_raw), n_va = length(y_va_raw))
```

------------------------------------------------------------------------

## PLS baseline

We fit a PLS regression model using `pls::plsr()` (if available). This is a strong baseline in spectroscopy.

```{r pls-fit}
if (!requireNamespace("pls", quietly = TRUE)) {
  message("Package 'pls' not available; skipping PLS section.")
} else {
  library(pls)

  datC <- data.frame(TotalCarbon = y_tr_raw)
  datV <- data.frame(TotalCarbon = y_va_raw)

  datC$spc <- I(X_tr_raw)
  datV$spc <- I(X_va_raw)

  maxc <- 30
  pls_mod <- plsr(TotalCarbon ~ spc, data = datC, method = "oscorespls", ncomp = maxc, validation = "CV")

  # pick ncomp at minimum RMSEP
  rmsep <- RMSEP(pls_mod)$val[1,1,-1]
  nc <- which.min(rmsep)
  nc

  yhat_tr_pls <- as.numeric(predict(pls_mod, ncomp = nc, newdata = datC))
  yhat_va_pls <- as.numeric(predict(pls_mod, ncomp = nc, newdata = datV))

  pls_cal <- cbind(Model="PLS", Split="Calibration", n=length(y_tr_raw), metrics(y_tr_raw, yhat_tr_pls))
  pls_val <- cbind(Model="PLS", Split="Validation",   n=length(y_va_raw), metrics(y_va_raw, yhat_va_pls))
}
```

------------------------------------------------------------------------

## soilVAE supervised VAE

### Check Python / TF / Keras

```{r vae-check}
has_py <- reticulate::py_available(initialize = FALSE)

has_tf <- FALSE
if (has_py) {
  try(reticulate::py_config(), silent = TRUE)
  has_tf <- reticulate::py_module_available("tensorflow") && reticulate::py_module_available("keras")
}

has_py
has_tf
```

If `has_tf` is `FALSE`, install TensorFlow/Keras in a Conda env and configure via:

``` r
# Example (run once, outside CRAN/CI):
# reticulate::install_miniconda()
# reticulate::conda_create("soilvae-tf", python_version = "3.11")
# reticulate::conda_install("soilvae-tf", packages = c("tensorflow", "keras", "numpy"), pip = TRUE)
# soilVAE::vae_configure(conda = "soilvae-tf")
```

### A) Standardize X and y (train-only)

**Why this matters:** Deep nets are very sensitive to scale. Standardizing often yields a big jump in R² / RMSE.

```{r scale-A}
scale_fit <- function(X) {
  X <- as.matrix(X)
  mu <- colMeans(X)
  sdv <- apply(X, 2, stats::sd)
  sdv[sdv == 0] <- 1
  list(mu = mu, sd = sdv)
}
scale_apply <- function(X, fit) {
  X <- as.matrix(X)
  sweep(sweep(X, 2, fit$mu, "-"), 2, fit$sd, "/")
}

xfit <- scale_fit(X_tr_raw)
X_tr <- scale_apply(X_tr_raw, xfit)
X_va <- scale_apply(X_va_raw, xfit)

# target scaling (optional but recommended)
y_mu <- mean(y_tr_raw)
y_sd <- stats::sd(y_tr_raw)
if (!is.finite(y_sd) || y_sd == 0) y_sd <- 1
y_tr <- (y_tr_raw - y_mu) / y_sd
y_va <- (y_va_raw - y_mu) / y_sd
```

### B) Wider tuning + multiple restarts (stability)

We do a small-but-meaningful grid, and for each configuration we run **several seeds**, keeping the best validation RMSE.

```{r vae-tuning-B, eval=FALSE}
# This chunk is disabled by default for CRAN/CI (TF may be unavailable and tuning can be slow).
# To run locally, set eval=TRUE and ensure TF/Keras available.
#
# IMPORTANT: The original full grid × multiple seeds can be *hundreds* of model fits.
# On CPU (or if TF is not properly configured), it can look like it is “stuck”.
# Below is a budgeted (random-search) tuning loop with:
#   - smaller default search budget (max_cfg)
#   - multiple restarts (seeds)
#   - per-run cleanup (clear_session + gc) to avoid memory build-up
#   - progress bar + checkpointing (resume-safe)

if (!has_tf) stop("TensorFlow/Keras not available; configure a Python env first.")

# Ensure the package uses your env (edit the name if needed)
soilVAE::vae_configure(conda = "soilvae-tf")

# -----------------------------
# 1) Candidate grid (expanded)
# -----------------------------
grid_all <- expand.grid(
  latent_dim = c(16L, 32L, 64L),
  dropout    = c(0.05, 0.10, 0.20),
  lr         = c(5e-4, 1e-3),
  beta_kl    = c(0.05, 0.10, 0.30, 0.50, 1.0),
  alpha_y    = c(1.0, 2.0, 5.0, 10.0),
  epochs     = c(120L),
  batch_size = c(64L),
  patience   = c(12L),
  stringsAsFactors = FALSE
)

# Hidden layers as list-cols (single architecture, tune later if needed)
grid_all$hidden_enc <- list(c(512L, 256L, 128L))
grid_all$hidden_dec <- list(c(128L, 256L, 512L))

# -----------------------------
# 2) Budget knobs (EDIT HERE)
# -----------------------------
max_cfg <- 24L          # number of configs to try (randomly sampled)
seeds   <- c(1L, 2L)    # restarts per config
checkpoint_path <- "soilVAE_tuning_checkpoint.rds"

# Randomly sample configs (random search)
set.seed(123)
if (nrow(grid_all) > max_cfg) {
  pick <- sample.int(nrow(grid_all), size = max_cfg)
  grid_vae <- grid_all[pick, , drop = FALSE]
} else {
  grid_vae <- grid_all
}

# -----------------------------
# 3) Resume from checkpoint
# -----------------------------
tuning_df <- NULL
if (file.exists(checkpoint_path)) {
  tuning_df <- try(readRDS(checkpoint_path), silent = TRUE)
  if (inherits(tuning_df, "try-error")) tuning_df <- NULL
}
if (is.null(tuning_df)) tuning_df <- data.frame()

# Helper: clear TF session to avoid memory buildup across many fits
tf_clear <- function() {
  if (reticulate::py_module_available("tensorflow")) {
    reticulate::py_run_string("import tensorflow as tf\ntf.keras.backend.clear_session()")
  }
  invisible(gc())
}

# -----------------------------
# 4) Run (random search × seeds)
# -----------------------------
total_runs <- nrow(grid_vae) * length(seeds)
pb <- utils::txtProgressBar(min = 0, max = total_runs, style = 3)
step <- 0L

for (i in seq_len(nrow(grid_vae))) {
  for (s in seeds) {
    step <- step + 1L
    utils::setTxtProgressBar(pb, step)

    # Skip if this (cfg, seed) already exists in the checkpoint
    if (nrow(tuning_df) > 0 &&
        all(c("cfg_id", "seed") %in% names(tuning_df)) &&
        any(tuning_df$cfg_id == i & tuning_df$seed == s, na.rm = TRUE)) {
      next
    }

    g <- grid_vae[i, , drop = FALSE]

    one <- try({
      res <- soilVAE::tune_vae_train_val(
        X_tr = X_tr, y_tr = y_tr,
        X_va = X_va, y_va = y_va,
        seed = s,
        grid_vae = g
      )
      df1 <- res$tuning_df
      df1$seed  <- s
      df1$cfg_id <- i
      df1
    }, silent = TRUE)

    if (inherits(one, "try-error")) {
      # record failure so you don't re-run it forever
      fail <- data.frame(cfg_id = i, seed = s,
                         RMSE_val = NA_real_, R2_val = NA_real_, RPIQ_val = NA_real_)
      tuning_df <- rbind(tuning_df, fail)
    } else {
      tuning_df <- rbind(tuning_df, one)
    }

    saveRDS(tuning_df, checkpoint_path)
    tf_clear()
  }
}
close(pb)

# -----------------------------
# 5) Select the best config
# -----------------------------
tuning_ok <- tuning_df[is.finite(tuning_df$RMSE_val), , drop = FALSE]
best_i <- which.min(tuning_ok$RMSE_val)
best <- tuning_ok[best_i, , drop = FALSE]
best
```

### Train final VAE with the best configuration

```{r vae-final, eval=FALSE}
if (!has_tf) stop("TensorFlow/Keras not available; configure a Python env first.")

cfg <- best

m_vae <- soilVAE::vae_build(
  input_dim  = ncol(X_tr),
  hidden_enc = as.integer(strsplit(cfg$hidden_enc_str, "-")[[1]]),
  hidden_dec = as.integer(strsplit(cfg$hidden_dec_str, "-")[[1]]),
  latent_dim = as.integer(cfg$latent_dim),
  dropout    = as.numeric(cfg$dropout),
  lr         = as.numeric(cfg$lr),
  beta_kl    = as.numeric(cfg$beta_kl),
  alpha_y    = as.numeric(cfg$alpha_y)
)

soilVAE::vae_fit(
  model = m_vae,
  X = X_tr, y = y_tr,
  X_val = X_va, y_val = y_va,
  epochs = as.integer(cfg$epochs),
  batch_size = as.integer(cfg$batch_size),
  patience = as.integer(cfg$patience),
  verbose = 0L
)

# Predictions (scaled space)
yhat_tr_s <- soilVAE::vae_predict(m_vae, X_tr)
yhat_va_s <- soilVAE::vae_predict(m_vae, X_va)

# Back-transform to original units
yhat_tr <- yhat_tr_s * y_sd + y_mu
yhat_va <- yhat_va_s * y_sd + y_mu

vae_cal <- cbind(Model="soilVAE", Split="Calibration", n=length(y_tr_raw), metrics(y_tr_raw, yhat_tr))
vae_val <- cbind(Model="soilVAE", Split="Validation",   n=length(y_va_raw), metrics(y_va_raw, yhat_va))
```

------------------------------------------------------------------------

## Compare metrics

```{r compare, eval=TRUE}
out <- NULL

# PLS rows (if computed)
if (exists("pls_cal")) out <- rbind(out, pls_cal, pls_val)

# VAE rows (if computed)
if (exists("vae_cal")) out <- rbind(out, vae_cal, vae_val)

if (is.null(out)) {
  message("No models were executed. If you want the VAE results, enable the eval=TRUE chunks and ensure TF/Keras are available.")
} else {
  out
}
```

------------------------------------------------------------------------

## Notes / troubleshooting

-   If `has_py` is FALSE: install Python or configure `reticulate` to point to an existing Python.
-   If `has_tf` is FALSE: install `tensorflow`, `keras`, and `numpy` in your env.
-   For reproducibility, keep the preprocessing and scaling fixed (no leakage).
-   To push performance further:
    -   increase grid breadth, especially `latent_dim`, `beta_kl`, `alpha_y`
    -   try deeper hidden layers (e.g., `c(1024, 512, 256)`)
    -   increase `epochs` + `patience` moderately
    -   run more restarts per configuration
